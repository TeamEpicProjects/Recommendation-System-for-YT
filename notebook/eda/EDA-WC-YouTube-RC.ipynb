{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "detailed-bracelet",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np # convert pictures to Arrays\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "import os\n",
    "import sys\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import warnings\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from PIL import Image \n",
    "from scipy.interpolate import griddata\n",
    "from PIL import Image\n",
    "\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "packed-washington",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For changing image resolution to get word cloud on images as mask\n",
    "\n",
    "def make_interpolated_image(nsamples, im,X,Y):\n",
    "    ix = np.random.randint(im.shape[1], size=nsamples)\n",
    "    iy = np.random.randint(im.shape[0], size=nsamples)\n",
    "    samples = im[iy,ix]\n",
    "    int_im = griddata((iy, ix), samples, (Y, X))\n",
    "    return int_im\n",
    "\n",
    "def change_image_resolution(low_res_img):\n",
    "    for image in low_res_img:\n",
    "        # Read in image and convert to greyscale array object\n",
    "        img_name = image\n",
    "        im = Image.open(img_name)\n",
    "        im = np.array(im.convert('L'))\n",
    "        \n",
    "        # A meshgrid of pixel coordinates\n",
    "        nx, ny = im.shape[1], im.shape[0]\n",
    "        X, Y = np.meshgrid(np.arange(0, nx, 1), np.arange(0, ny, 1))\n",
    "        \n",
    "        # Create a figure of nrows x ncols subplots, and orient it appropriately\n",
    "        # for the aspect ratio of the image.\n",
    "        \n",
    "        plt.figure(figsize = (6,6), facecolor = None)\n",
    "        \n",
    "        # Convert an integer i to coordinates in the ax array\n",
    "        get_indices = lambda i: (i // nrows, i % ncols)\n",
    "        \n",
    "        # Sample 100, 1,000, 10,000 and 100,000 points and plot the interpolated\n",
    "        # images in the figure\n",
    "        nsamples = 1000000\n",
    "        # axes = ax[get_indices(i)]\n",
    "        plt.imshow(make_interpolated_image(nsamples,im,X,Y),\n",
    "                              cmap=plt.get_cmap('Greys_r'))\n",
    "        # plt.set_xticks([])\n",
    "        # plt.set_yticks([])\n",
    "        # plt.title('nsamples = {0:d}'.format(nsamples))\n",
    "        filestem = os.path.splitext(os.path.basename(img_name))[0]\n",
    "        plt.savefig(image, dpi=300)\n",
    "\n",
    "# low_res_img = ['HimachalPradesh','Odisha','Rajasthan']\n",
    "\n",
    "\n",
    "# low_res_img = [r'C:\\\\Users\\\\vgarg\\\\Downloads\\\\3IndianSatesYouTubeData\\\\' + image +'.png' for image in low_res_img]\n",
    "\n",
    "## uncomment below when need high resolutiion image\n",
    "# change_image_resolution(low_res_img) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocal-recovery",
   "metadata": {},
   "source": [
    "## Word cloud for all Indian states and for whole India dataset for YouTube videos 'Title' 'Description' and 'Tags'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "molecular-sleep",
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_files = ['IndianStateMapImage','IndianStateYouTubeData']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "gentle-burden",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up():\n",
    "    dir_path = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "    file = os.listdir(dir_path)\n",
    "    for images in file:\n",
    "        if images.endswith(\".png\") or images.endswith(\".csv\") :\n",
    "            os.remove(os.path.join(dir_path, images))\n",
    "clean_up()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "decreased-jewel",
   "metadata": {},
   "outputs": [],
   "source": [
    "## get all state data file name and image file name and store \n",
    "def get_state_data(zip_files):\n",
    "    des_path = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "    print(des_path)\n",
    "\n",
    "    fileDirectory = os.path.dirname(des_path)\n",
    "    parentDirectory = os.path.dirname(fileDirectory)\n",
    "    #Navigate to Strings directory\n",
    "    source_path = os.path.join(parentDirectory, 'data', 'raw')\n",
    "    \n",
    "    for zip_file in zip_files:      \n",
    "        with zipfile.ZipFile(source_path+'\\\\'+zip_file+'.zip',\"r\") as zip_ref:\n",
    "            zip_ref.extractall(des_path)\n",
    "    states=os.listdir(des_path)\n",
    "    for state in states[:]:\n",
    "        if not(state.endswith(\".png\")):\n",
    "            states.remove(state)\n",
    "    states = [state.strip('.png') for state in states]\n",
    "    image_files = []\n",
    "    data_files = []\n",
    "    for state in states:\n",
    "        image_files.append(des_path+'\\\\'+state+'.png')\n",
    "        data_files.append(des_path+'\\\\'+'youTubeSearchList'+state+'.csv')\n",
    "    return image_files, data_files, states  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a1773f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(text):\n",
    "    \n",
    "    # lowercase\n",
    "    text=text.lower()\n",
    "    \n",
    "    #remove tags\n",
    "    text=re.sub(\"\",\"\",text)\n",
    "    \n",
    "    # remove special characters and digits\n",
    "    text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def get_stop_words(stop_file_path):\n",
    "    \"\"\"load stop words \"\"\"\n",
    "    \n",
    "    with open(stop_file_path, 'r', encoding=\"utf-8\") as f:\n",
    "        stopwords = f.readlines()\n",
    "        stop_set = set(m.strip() for m in stopwords)\n",
    "        return frozenset(stop_set)\n",
    "    \n",
    "def sort_coo(coo_matrix):\n",
    "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
    "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
    "\n",
    "def extract_topn_from_vector(feature_names, sorted_items, topn=5):\n",
    "    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n",
    "    \n",
    "    #use only topn items from vector\n",
    "    sorted_items = sorted_items[:topn]\n",
    "\n",
    "    score_vals = []\n",
    "    feature_vals = []\n",
    "    \n",
    "    # word index and corresponding tf-idf score\n",
    "    for idx, score in sorted_items:\n",
    "        \n",
    "        #keep track of feature name and its corresponding score\n",
    "        score_vals.append(round(score, 3))\n",
    "        feature_vals.append(feature_names[idx])\n",
    "\n",
    "    #create a tuples of feature,score\n",
    "    #results = zip(feature_vals,score_vals)\n",
    "    results= {}\n",
    "    for idx in range(len(feature_vals)):\n",
    "        results[feature_vals[idx]]=score_vals[idx]\n",
    "    \n",
    "    return results\n",
    "\n",
    "def tfidf(filename, image):\n",
    "    df = pd.read_csv(filename,index_col=[0])\n",
    "    print(\"Data Read\")\n",
    "    df['title'] = df['title'].convert_dtypes()\n",
    "    df['description'] = df['description'].convert_dtypes()\n",
    "    df['tags'] = df['tags'].convert_dtypes()\n",
    "    df = df.dropna()\n",
    "    df = df.reset_index(drop=True)\n",
    "    print(\"Null values dropped\")\n",
    "    df['text'] = df['title'] + df['description'] + df['tags']\n",
    "\n",
    "    df = df[:10] ## TO BE DELETE, just for POC coding\n",
    "\n",
    "    df['text'] = df['text'].apply(lambda x:pre_process(x))\n",
    "    \n",
    "    des_path = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "    fileDirectory = os.path.dirname(des_path)\n",
    "    parentDirectory = os.path.dirname(fileDirectory)\n",
    "    #Navigate to Strings directory\n",
    "    source_path = os.path.join(parentDirectory, 'notebook','Preprocessing', 'resources')\n",
    "    source_path = source_path+ '\\\\stopwords.txt'\n",
    "    #load a set of stop words\n",
    "    stopwords=get_stop_words(source_path)\n",
    "\n",
    "    #get the text column \n",
    "    docs=df['text'].tolist()\n",
    "\n",
    "    #create a vocabulary of words, \n",
    "    #ignore words that appear in 85% of documents, \n",
    "    #eliminate stop words\n",
    "    cv=CountVectorizer(max_df=0.85,stop_words=stopwords)\n",
    "    word_count_vector=cv.fit_transform(docs)\n",
    "#     print(list(cv.vocabulary_.keys())[:10])\n",
    "    \n",
    "    tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "    tfidf_transformer.fit(word_count_vector)\n",
    "    # you only needs to do this once, this is a mapping of index to \n",
    "    feature_names=cv.get_feature_names()\n",
    "    i = 0\n",
    "    tex_list = df['text'].tolist()\n",
    "    tf_idf_vector_list = []\n",
    "    for i in range(len(tex_list)):\n",
    "        tf_idf_vector=tfidf_transformer.transform(cv.transform([tex_list[i]]))\n",
    "        i = i+1\n",
    "        tf_idf_vector_list.append(tf_idf_vector)\n",
    "    print(\"Tf-idf Generated\")\n",
    "    \n",
    "    sorted_items_list=[]\n",
    "    i=0\n",
    "    for i in range(len(tf_idf_vector_list)):\n",
    "        sorted_items=sort_coo(tf_idf_vector_list[i].tocoo())\n",
    "        i = i+1\n",
    "        sorted_items_list.append(sorted_items)\n",
    "    print(\"Sorting Complete\")\n",
    "    keyword_list = []\n",
    "    i=0\n",
    "    for i in range(len(sorted_items_list)):\n",
    "        keywords=extract_topn_from_vector(feature_names,sorted_items_list[i],5)\n",
    "        i = i+1\n",
    "        keyword_list.append(keywords)\n",
    "    print(\"Top 5 extraction done\")\n",
    "    count_list = []\n",
    "    comment_words=''\n",
    "    for i  in range(len(keyword_list)):\n",
    "        for k in keyword_list[i]:\n",
    "            count_list.append(k)\n",
    "#             if((i%10000)==0):\n",
    "#                 print(\"Keywords added\",i)\n",
    "            comment_words += \"\".join(k)+\" \"\n",
    "    print(\"Keywords text file create\")\n",
    "    \n",
    "    mask = np.array(Image.open(image))# Path for Image of the state\n",
    "    \n",
    "    wordcloud = WordCloud(width = 2000, height = 1600,\n",
    "                background_color ='white',\n",
    "                stopwords = stopwords,\n",
    "                min_font_size = 5,\n",
    "                mask = mask,\n",
    "                collocations=False).generate(comment_words)\n",
    "    return wordcloud\n",
    "#     plot the WordCloud image                      \n",
    "#     plt.figure(figsize = (8, 8), facecolor = None)\n",
    "#     plt.imshow(wordcloud)\n",
    "#     plt.axis(\"off\")\n",
    "#     plt.tight_layout(pad = 0)\n",
    "\n",
    "#     plt.imshow(wordcloud, interpolation='bilinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "defined-fabric",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vgarg\\Documents\\GitHub\\Recommendation-System-for-YT\\notebook\\eda\n",
      "['C:\\\\Users\\\\vgarg\\\\Documents\\\\GitHub\\\\Recommendation-System-for-YT\\\\notebook\\\\eda\\\\AndamanandNicobarIslands.png', 'C:\\\\Users\\\\vgarg\\\\Documents\\\\GitHub\\\\Recommendation-System-for-YT\\\\notebook\\\\eda\\\\AndhraPradesh.png', 'C:\\\\Users\\\\vgarg\\\\Documents\\\\GitHub\\\\Recommendation-System-for-YT\\\\notebook\\\\eda\\\\ArunachalPradesh.png', 'C:\\\\Users\\\\vgarg\\\\Documents\\\\GitHub\\\\Recommendation-System-for-YT\\\\notebook\\\\eda\\\\Assam.png', 'C:\\\\Users\\\\vgarg\\\\Documents\\\\GitHub\\\\Recommendation-System-for-YT\\\\notebook\\\\eda\\\\Bihar.png', 'C:\\\\Users\\\\vgarg\\\\Documents\\\\GitHub\\\\Recommendation-System-for-YT\\\\notebook\\\\eda\\\\Chandigarh.png', 'C:\\\\Users\\\\vgarg\\\\Documents\\\\GitHub\\\\Recommendation-System-for-YT\\\\notebook\\\\eda\\\\Chattishgarh.png', 'C:\\\\Users\\\\vgarg\\\\Documents\\\\GitHub\\\\Recommendation-System-for-YT\\\\notebook\\\\eda\\\\Goa.png', 'C:\\\\Users\\\\vgarg\\\\Documents\\\\GitHub\\\\Recommendation-System-for-YT\\\\notebook\\\\eda\\\\Gujarat.png', 'C:\\\\Users\\\\vgarg\\\\Documents\\\\GitHub\\\\Recommendation-System-for-YT\\\\notebook\\\\eda\\\\Haryana.png', 'C:\\\\Users\\\\vgarg\\\\Documents\\\\GitHub\\\\Recommendation-System-for-YT\\\\notebook\\\\eda\\\\HimachalPradesh.png', 'C:\\\\Users\\\\vgarg\\\\Documents\\\\GitHub\\\\Recommendation-System-for-YT\\\\notebook\\\\eda\\\\India.png', 'C:\\\\Users\\\\vgarg\\\\Documents\\\\GitHub\\\\Recommendation-System-for-YT\\\\notebook\\\\eda\\\\Jammu & Kashmir.png', 'C:\\\\Users\\\\vgarg\\\\Documents\\\\GitHub\\\\Recommendation-System-for-YT\\\\notebook\\\\eda\\\\Jharkhand.png', 'C:\\\\Users\\\\vgarg\\\\Documents\\\\GitHub\\\\Recommendation-System-for-YT\\\\notebook\\\\eda\\\\Karntaka.png', 'C:\\\\Users\\\\vgarg\\\\Documents\\\\GitHub\\\\Recommendation-System-for-YT\\\\notebook\\\\eda\\\\Kerala.png', 'C:\\\\Users\\\\vgarg\\\\Documents\\\\GitHub\\\\Recommendation-System-for-YT\\\\notebook\\\\eda\\\\lakshadwee.png', 'C:\\\\Users\\\\vgarg\\\\Documents\\\\GitHub\\\\Recommendation-System-for-YT\\\\notebook\\\\eda\\\\MadyaPradesh.png', 'C:\\\\Users\\\\vgarg\\\\Documents\\\\GitHub\\\\Recommendation-System-for-YT\\\\notebook\\\\eda\\\\Maharastra.png', 'C:\\\\Users\\\\vgarg\\\\Documents\\\\GitHub\\\\Recommendation-System-for-YT\\\\notebook\\\\eda\\\\Manipur.png', 'C:\\\\Users\\\\vgarg\\\\Documents\\\\GitHub\\\\Recommendation-System-for-YT\\\\notebook\\\\eda\\\\Meghalaya.png', 'C:\\\\Users\\\\vgarg\\\\Documents\\\\GitHub\\\\Recommendation-System-for-YT\\\\notebook\\\\eda\\\\Mizoram.png', 'C:\\\\Users\\\\vgarg\\\\Documents\\\\GitHub\\\\Recommendation-System-for-YT\\\\notebook\\\\eda\\\\Nagaland.png', 'C:\\\\Users\\\\vgarg\\\\Documents\\\\GitHub\\\\Recommendation-System-for-YT\\\\notebook\\\\eda\\\\Odisha.png', 'C:\\\\Users\\\\vgarg\\\\Documents\\\\GitHub\\\\Recommendation-System-for-YT\\\\notebook\\\\eda\\\\Punjab.png', 'C:\\\\Users\\\\vgarg\\\\Documents\\\\GitHub\\\\Recommendation-System-for-YT\\\\notebook\\\\eda\\\\Rajastha.png', 'C:\\\\Users\\\\vgarg\\\\Documents\\\\GitHub\\\\Recommendation-System-for-YT\\\\notebook\\\\eda\\\\Sikkim.png', 'C:\\\\Users\\\\vgarg\\\\Documents\\\\GitHub\\\\Recommendation-System-for-YT\\\\notebook\\\\eda\\\\TamilNadu.png', 'C:\\\\Users\\\\vgarg\\\\Documents\\\\GitHub\\\\Recommendation-System-for-YT\\\\notebook\\\\eda\\\\Telangana.png', 'C:\\\\Users\\\\vgarg\\\\Documents\\\\GitHub\\\\Recommendation-System-for-YT\\\\notebook\\\\eda\\\\Tripura.png', 'C:\\\\Users\\\\vgarg\\\\Documents\\\\GitHub\\\\Recommendation-System-for-YT\\\\notebook\\\\eda\\\\Uttarakhand.png', 'C:\\\\Users\\\\vgarg\\\\Documents\\\\GitHub\\\\Recommendation-System-for-YT\\\\notebook\\\\eda\\\\UttarPradesh.png', 'C:\\\\Users\\\\vgarg\\\\Documents\\\\GitHub\\\\Recommendation-System-for-YT\\\\notebook\\\\eda\\\\WestBengal.png']\n",
      "['C:\\\\Users\\\\vgarg\\\\Documents\\\\GitHub\\\\Recommendation-System-for-YT\\\\notebook\\\\eda\\\\youTubeSearchListAndamanandNicobarIslands.csv', 'C:\\\\Users\\\\vgarg\\\\Documents\\\\GitHub\\\\Recommendation-System-for-YT\\\\notebook\\\\eda\\\\youTubeSearchListAndhraPradesh.csv', 'C:\\\\Users\\\\vgarg\\\\Documents\\\\GitHub\\\\Recommendation-System-for-YT\\\\notebook\\\\eda\\\\youTubeSearchListArunachalPradesh.csv', 'C:\\\\Users\\\\vgarg\\\\Documents\\\\GitHub\\\\Recommendation-System-for-YT\\\\notebook\\\\eda\\\\youTubeSearchListAssam.csv', 'C:\\\\Users\\\\vgarg\\\\Documents\\\\GitHub\\\\Recommendation-System-for-YT\\\\notebook\\\\eda\\\\youTubeSearchListBihar.csv', 'C:\\\\Users\\\\vgarg\\\\Documents\\\\GitHub\\\\Recommendation-System-for-YT\\\\notebook\\\\eda\\\\youTubeSearchListChandigarh.csv', 'C:\\\\Users\\\\vgarg\\\\Documents\\\\GitHub\\\\Recommendation-System-for-YT\\\\notebook\\\\eda\\\\youTubeSearchListChattishgarh.csv', 'C:\\\\Users\\\\vgarg\\\\Documents\\\\GitHub\\\\Recommendation-System-for-YT\\\\notebook\\\\eda\\\\youTubeSearchListGoa.csv', 'C:\\\\Users\\\\vgarg\\\\Documents\\\\GitHub\\\\Recommendation-System-for-YT\\\\notebook\\\\eda\\\\youTubeSearchListGujarat.csv', 'C:\\\\Users\\\\vgarg\\\\Documents\\\\GitHub\\\\Recommendation-System-for-YT\\\\notebook\\\\eda\\\\youTubeSearchListHaryana.csv', 'C:\\\\Users\\\\vgarg\\\\Documents\\\\GitHub\\\\Recommendation-System-for-YT\\\\notebook\\\\eda\\\\youTubeSearchListHimachalPradesh.csv', 'C:\\\\Users\\\\vgarg\\\\Documents\\\\GitHub\\\\Recommendation-System-for-YT\\\\notebook\\\\eda\\\\youTubeSearchListIndia.csv', 'C:\\\\Users\\\\vgarg\\\\Documents\\\\GitHub\\\\Recommendation-System-for-YT\\\\notebook\\\\eda\\\\youTubeSearchListJammu & Kashmir.csv', 'C:\\\\Users\\\\vgarg\\\\Documents\\\\GitHub\\\\Recommendation-System-for-YT\\\\notebook\\\\eda\\\\youTubeSearchListJharkhand.csv', 'C:\\\\Users\\\\vgarg\\\\Documents\\\\GitHub\\\\Recommendation-System-for-YT\\\\notebook\\\\eda\\\\youTubeSearchListKarntaka.csv', 'C:\\\\Users\\\\vgarg\\\\Documents\\\\GitHub\\\\Recommendation-System-for-YT\\\\notebook\\\\eda\\\\youTubeSearchListKerala.csv', 'C:\\\\Users\\\\vgarg\\\\Documents\\\\GitHub\\\\Recommendation-System-for-YT\\\\notebook\\\\eda\\\\youTubeSearchListlakshadwee.csv', 'C:\\\\Users\\\\vgarg\\\\Documents\\\\GitHub\\\\Recommendation-System-for-YT\\\\notebook\\\\eda\\\\youTubeSearchListMadyaPradesh.csv', 'C:\\\\Users\\\\vgarg\\\\Documents\\\\GitHub\\\\Recommendation-System-for-YT\\\\notebook\\\\eda\\\\youTubeSearchListMaharastra.csv', 'C:\\\\Users\\\\vgarg\\\\Documents\\\\GitHub\\\\Recommendation-System-for-YT\\\\notebook\\\\eda\\\\youTubeSearchListManipur.csv', 'C:\\\\Users\\\\vgarg\\\\Documents\\\\GitHub\\\\Recommendation-System-for-YT\\\\notebook\\\\eda\\\\youTubeSearchListMeghalaya.csv', 'C:\\\\Users\\\\vgarg\\\\Documents\\\\GitHub\\\\Recommendation-System-for-YT\\\\notebook\\\\eda\\\\youTubeSearchListMizoram.csv', 'C:\\\\Users\\\\vgarg\\\\Documents\\\\GitHub\\\\Recommendation-System-for-YT\\\\notebook\\\\eda\\\\youTubeSearchListNagaland.csv', 'C:\\\\Users\\\\vgarg\\\\Documents\\\\GitHub\\\\Recommendation-System-for-YT\\\\notebook\\\\eda\\\\youTubeSearchListOdisha.csv', 'C:\\\\Users\\\\vgarg\\\\Documents\\\\GitHub\\\\Recommendation-System-for-YT\\\\notebook\\\\eda\\\\youTubeSearchListPunjab.csv', 'C:\\\\Users\\\\vgarg\\\\Documents\\\\GitHub\\\\Recommendation-System-for-YT\\\\notebook\\\\eda\\\\youTubeSearchListRajastha.csv', 'C:\\\\Users\\\\vgarg\\\\Documents\\\\GitHub\\\\Recommendation-System-for-YT\\\\notebook\\\\eda\\\\youTubeSearchListSikkim.csv', 'C:\\\\Users\\\\vgarg\\\\Documents\\\\GitHub\\\\Recommendation-System-for-YT\\\\notebook\\\\eda\\\\youTubeSearchListTamilNadu.csv', 'C:\\\\Users\\\\vgarg\\\\Documents\\\\GitHub\\\\Recommendation-System-for-YT\\\\notebook\\\\eda\\\\youTubeSearchListTelangana.csv', 'C:\\\\Users\\\\vgarg\\\\Documents\\\\GitHub\\\\Recommendation-System-for-YT\\\\notebook\\\\eda\\\\youTubeSearchListTripura.csv', 'C:\\\\Users\\\\vgarg\\\\Documents\\\\GitHub\\\\Recommendation-System-for-YT\\\\notebook\\\\eda\\\\youTubeSearchListUttarakhand.csv', 'C:\\\\Users\\\\vgarg\\\\Documents\\\\GitHub\\\\Recommendation-System-for-YT\\\\notebook\\\\eda\\\\youTubeSearchListUttarPradesh.csv', 'C:\\\\Users\\\\vgarg\\\\Documents\\\\GitHub\\\\Recommendation-System-for-YT\\\\notebook\\\\eda\\\\youTubeSearchListWestBengal.csv']\n"
     ]
    }
   ],
   "source": [
    "image_files,data_files, states = get_state_data(zip_files)\n",
    "print(image_files)\n",
    "print(data_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "amended-bailey",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data:  C:\\Users\\vgarg\\Documents\\GitHub\\Recommendation-System-for-YT\\notebook\\eda\\youTubeSearchListAndamanandNicobarIslands.csv\n",
      "Image:  C:\\Users\\vgarg\\Documents\\GitHub\\Recommendation-System-for-YT\\notebook\\eda\\AndamanandNicobarIslands.png\n",
      "Data Read\n",
      "Null values dropped\n",
      "Tf-idf Generated\n",
      "Sorting Complete\n",
      "Top 5 extraction done\n",
      "Keywords text file create\n",
      "Data:  C:\\Users\\vgarg\\Documents\\GitHub\\Recommendation-System-for-YT\\notebook\\eda\\youTubeSearchListAndhraPradesh.csv\n",
      "Image:  C:\\Users\\vgarg\\Documents\\GitHub\\Recommendation-System-for-YT\\notebook\\eda\\AndhraPradesh.png\n",
      "Data Read\n",
      "Null values dropped\n",
      "Tf-idf Generated\n",
      "Sorting Complete\n",
      "Top 5 extraction done\n",
      "Keywords text file create\n",
      "Data:  C:\\Users\\vgarg\\Documents\\GitHub\\Recommendation-System-for-YT\\notebook\\eda\\youTubeSearchListArunachalPradesh.csv\n",
      "Image:  C:\\Users\\vgarg\\Documents\\GitHub\\Recommendation-System-for-YT\\notebook\\eda\\ArunachalPradesh.png\n",
      "Data Read\n",
      "Null values dropped\n",
      "Tf-idf Generated\n",
      "Sorting Complete\n",
      "Top 5 extraction done\n",
      "Keywords text file create\n",
      "Data:  C:\\Users\\vgarg\\Documents\\GitHub\\Recommendation-System-for-YT\\notebook\\eda\\youTubeSearchListAssam.csv\n",
      "Image:  C:\\Users\\vgarg\\Documents\\GitHub\\Recommendation-System-for-YT\\notebook\\eda\\Assam.png\n",
      "Data Read\n",
      "Null values dropped\n",
      "Tf-idf Generated\n",
      "Sorting Complete\n",
      "Top 5 extraction done\n",
      "Keywords text file create\n",
      "Data:  C:\\Users\\vgarg\\Documents\\GitHub\\Recommendation-System-for-YT\\notebook\\eda\\youTubeSearchListBihar.csv\n",
      "Image:  C:\\Users\\vgarg\\Documents\\GitHub\\Recommendation-System-for-YT\\notebook\\eda\\Bihar.png\n",
      "Data Read\n",
      "Null values dropped\n",
      "Tf-idf Generated\n",
      "Sorting Complete\n",
      "Top 5 extraction done\n",
      "Keywords text file create\n",
      "Data:  C:\\Users\\vgarg\\Documents\\GitHub\\Recommendation-System-for-YT\\notebook\\eda\\youTubeSearchListChandigarh.csv\n",
      "Image:  C:\\Users\\vgarg\\Documents\\GitHub\\Recommendation-System-for-YT\\notebook\\eda\\Chandigarh.png\n",
      "Data Read\n",
      "Null values dropped\n",
      "Tf-idf Generated\n",
      "Sorting Complete\n",
      "Top 5 extraction done\n",
      "Keywords text file create\n",
      "Data:  C:\\Users\\vgarg\\Documents\\GitHub\\Recommendation-System-for-YT\\notebook\\eda\\youTubeSearchListChattishgarh.csv\n",
      "Image:  C:\\Users\\vgarg\\Documents\\GitHub\\Recommendation-System-for-YT\\notebook\\eda\\Chattishgarh.png\n",
      "Data Read\n",
      "Null values dropped\n",
      "Tf-idf Generated\n",
      "Sorting Complete\n",
      "Top 5 extraction done\n",
      "Keywords text file create\n",
      "Data:  C:\\Users\\vgarg\\Documents\\GitHub\\Recommendation-System-for-YT\\notebook\\eda\\youTubeSearchListGoa.csv\n",
      "Image:  C:\\Users\\vgarg\\Documents\\GitHub\\Recommendation-System-for-YT\\notebook\\eda\\Goa.png\n",
      "Data Read\n",
      "Null values dropped\n",
      "Tf-idf Generated\n",
      "Sorting Complete\n",
      "Top 5 extraction done\n",
      "Keywords text file create\n",
      "Data:  C:\\Users\\vgarg\\Documents\\GitHub\\Recommendation-System-for-YT\\notebook\\eda\\youTubeSearchListGujarat.csv\n",
      "Image:  C:\\Users\\vgarg\\Documents\\GitHub\\Recommendation-System-for-YT\\notebook\\eda\\Gujarat.png\n",
      "Data Read\n",
      "Null values dropped\n",
      "Tf-idf Generated\n",
      "Sorting Complete\n",
      "Top 5 extraction done\n",
      "Keywords text file create\n",
      "Data:  C:\\Users\\vgarg\\Documents\\GitHub\\Recommendation-System-for-YT\\notebook\\eda\\youTubeSearchListHaryana.csv\n",
      "Image:  C:\\Users\\vgarg\\Documents\\GitHub\\Recommendation-System-for-YT\\notebook\\eda\\Haryana.png\n",
      "Data Read\n",
      "Null values dropped\n",
      "Tf-idf Generated\n",
      "Sorting Complete\n",
      "Top 5 extraction done\n",
      "Keywords text file create\n",
      "Data:  C:\\Users\\vgarg\\Documents\\GitHub\\Recommendation-System-for-YT\\notebook\\eda\\youTubeSearchListHimachalPradesh.csv\n",
      "Image:  C:\\Users\\vgarg\\Documents\\GitHub\\Recommendation-System-for-YT\\notebook\\eda\\HimachalPradesh.png\n",
      "Data Read\n",
      "Null values dropped\n",
      "Tf-idf Generated\n",
      "Sorting Complete\n",
      "Top 5 extraction done\n",
      "Keywords text file create\n",
      "Data:  C:\\Users\\vgarg\\Documents\\GitHub\\Recommendation-System-for-YT\\notebook\\eda\\youTubeSearchListIndia.csv\n",
      "Image:  C:\\Users\\vgarg\\Documents\\GitHub\\Recommendation-System-for-YT\\notebook\\eda\\India.png\n",
      "Data Read\n",
      "Null values dropped\n",
      "Tf-idf Generated\n",
      "Sorting Complete\n",
      "Top 5 extraction done\n",
      "Keywords text file create\n",
      "Data:  C:\\Users\\vgarg\\Documents\\GitHub\\Recommendation-System-for-YT\\notebook\\eda\\youTubeSearchListJammu & Kashmir.csv\n",
      "Image:  C:\\Users\\vgarg\\Documents\\GitHub\\Recommendation-System-for-YT\\notebook\\eda\\Jammu & Kashmir.png\n",
      "Data Read\n",
      "Null values dropped\n",
      "Tf-idf Generated\n",
      "Sorting Complete\n",
      "Top 5 extraction done\n",
      "Keywords text file create\n",
      "Data:  C:\\Users\\vgarg\\Documents\\GitHub\\Recommendation-System-for-YT\\notebook\\eda\\youTubeSearchListJharkhand.csv\n",
      "Image:  C:\\Users\\vgarg\\Documents\\GitHub\\Recommendation-System-for-YT\\notebook\\eda\\Jharkhand.png\n",
      "Data Read\n",
      "Null values dropped\n",
      "Tf-idf Generated\n",
      "Sorting Complete\n",
      "Top 5 extraction done\n",
      "Keywords text file create\n",
      "Data:  C:\\Users\\vgarg\\Documents\\GitHub\\Recommendation-System-for-YT\\notebook\\eda\\youTubeSearchListKarntaka.csv\n",
      "Image:  C:\\Users\\vgarg\\Documents\\GitHub\\Recommendation-System-for-YT\\notebook\\eda\\Karntaka.png\n",
      "Data Read\n",
      "Null values dropped\n",
      "Tf-idf Generated\n",
      "Sorting Complete\n",
      "Top 5 extraction done\n",
      "Keywords text file create\n",
      "Data:  C:\\Users\\vgarg\\Documents\\GitHub\\Recommendation-System-for-YT\\notebook\\eda\\youTubeSearchListKerala.csv\n",
      "Image:  C:\\Users\\vgarg\\Documents\\GitHub\\Recommendation-System-for-YT\\notebook\\eda\\Kerala.png\n",
      "Data Read\n",
      "Null values dropped\n",
      "Tf-idf Generated\n",
      "Sorting Complete\n",
      "Top 5 extraction done\n",
      "Keywords text file create\n",
      "Data:  C:\\Users\\vgarg\\Documents\\GitHub\\Recommendation-System-for-YT\\notebook\\eda\\youTubeSearchListlakshadwee.csv\n",
      "Image:  C:\\Users\\vgarg\\Documents\\GitHub\\Recommendation-System-for-YT\\notebook\\eda\\lakshadwee.png\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\vgarg\\\\Documents\\\\GitHub\\\\Recommendation-System-for-YT\\\\notebook\\\\eda\\\\youTubeSearchListlakshadwee.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-29cc4852b4e3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Data: '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Image: '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mstates_wordcloud\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtfidf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mimage_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Total No of state for world cloud:-\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates_wordcloud\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-23-ee3cce622275>\u001b[0m in \u001b[0;36mtfidf\u001b[1;34m(filename, image)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex_col\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Data Read\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'title'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'title'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_dtypes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    609\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 610\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    611\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    461\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 462\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    464\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1048\u001b[0m             )\n\u001b[0;32m   1049\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1050\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1051\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1052\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1865\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1866\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1867\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1868\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1869\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"storage_options\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"encoding\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"memory_map\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"compression\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m   1360\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHanldes\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1361\u001b[0m         \"\"\"\n\u001b[1;32m-> 1362\u001b[1;33m         self.handles = get_handle(\n\u001b[0m\u001b[0;32m   1363\u001b[0m             \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1364\u001b[0m             \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    640\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"replace\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 642\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    644\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\vgarg\\\\Documents\\\\GitHub\\\\Recommendation-System-for-YT\\\\notebook\\\\eda\\\\youTubeSearchListlakshadwee.csv'"
     ]
    }
   ],
   "source": [
    "states_wordcloud = []\n",
    "for image_file, data_file in zip(image_files,data_files):\n",
    "    print('Data: ', data_file)\n",
    "    print('Image: ', image_file)\n",
    "    states_wordcloud.append(tfidf(data_file,image_file))\n",
    "print(\"Total No of state for world cloud:-\",len(states_wordcloud))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protective-benchmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the WordCloud image    \n",
    "for state_wordcloud, state in zip(states_wordcloud, states):\n",
    "    plt.figure(figsize = (12,12), facecolor = None)\n",
    "    plt.imshow(state_wordcloud)\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout(pad = 0)\n",
    "    plt.title(state)\n",
    "    plt.savefig(state+'_wordcloud.png', format = 'png', dpi=300)\n",
    "    plt.show()\n",
    "# plt.imshow(states_wordcloud[1, interpolation='bilinear')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lucky-marker",
   "metadata": {},
   "source": [
    "## Wordcloud based on top liked and disliked count for youtTube india data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "separate-shock",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "local-mortgage",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# word count\n",
    "i = 0\n",
    "for i in range(2):\n",
    "    if i == 0:\n",
    "        df_eda.sort_values(['likeCount'], ascending=False,axis=0,inplace=True)\n",
    "    else:\n",
    "        df_eda.sort_values(['dislikeCount'], ascending=False,axis=0,inplace=True)\n",
    "    \n",
    "    comment_words = ''\n",
    "    stopwords = set(STOPWORDS)\n",
    "    \n",
    "    # iterate through the csv file\n",
    "    for val in df.description.iloc[0:10000]:\n",
    "         \n",
    "        # typecaste each val to string\n",
    "        val = str(val)\n",
    "     \n",
    "        # split the value\n",
    "        tokens = val.split()\n",
    "        \n",
    "        # Converts each token into lowercase\n",
    "        for i in range(len(tokens)):\n",
    "            tokens[i] = tokens[i].lower()\n",
    "         \n",
    "        comment_words += \" \".join(tokens)+\" \"\n",
    "     \n",
    "    wordcloud = WordCloud(width = 800, height = 800,\n",
    "                    background_color ='white',\n",
    "                    stopwords = stopwords,\n",
    "                    min_font_size = 10).generate(comment_words)\n",
    "     \n",
    "    # plot the WordCloud image                      \n",
    "    plt.figure(figsize = (8, 8), facecolor = None)\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout(pad = 0)\n",
    "    plt.title('Top Liked/Disliked Description')\n",
    "         \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corrected-mission",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "military-large",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
